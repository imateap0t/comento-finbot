import hashlib
import os
import sqlite3
import time
import threading
from io import BytesIO

import streamlit as st
from datetime import datetime
from dotenv import load_dotenv

# ====== LLM & LangChain (LangChain 0.3.27) ======
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever
from langchain_core.callbacks import BaseCallbackHandler
from langchain.retrievers import EnsembleRetriever

try:
    from langchain_community.retrievers import BM25Retriever
    BM25_AVAILABLE = True
except ImportError:
    BM25_AVAILABLE = False

# ====== PDF/ì‹œê°í™” ======
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.pagesizes import A4
from wordcloud import WordCloud, STOPWORDS

# ====== ê¸°ë³¸ ì„¤ì • ======
load_dotenv()

st.set_page_config(page_title="ê¸ˆìœµ ì±—ë´‡", page_icon="ğŸ’¹")

def measure_response_time(func):
    """ì‘ë‹µ ì‹œê°„ ì¸¡ì • í•¨ìˆ˜"""
    start_time = time.time()
    
    try:
        result = func()
        success = True
    except Exception as e:
        success = False
        st.session_state.performance_metrics["errors"] += 1
        raise e
    finally:
        end_time = time.time()
        response_time = end_time - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        st.session_state.performance_metrics["query_count"] += 1
        st.session_state.performance_metrics["total_time"] += response_time
        st.session_state.performance_metrics["response_times"].append({
            "time": response_time,
            "timestamp": datetime.now(),
            "success": success
        })
        
        # ì‹¤ì‹œê°„ ì„±ëŠ¥ í‘œì‹œ
        avg_time = st.session_state.performance_metrics["total_time"] / st.session_state.performance_metrics["query_count"]
        
        if response_time > 10:
            st.warning(f"âš ï¸ ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ (í‰ê· : {avg_time:.2f}ì´ˆ)")
        else:
            st.info(f"âš¡ ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ (í‰ê· : {avg_time:.2f}ì´ˆ)")
    
    return result

if "performance_metrics" not in st.session_state:
    st.session_state.performance_metrics = {
        "query_count": 0,
        "total_time": 0,
        "response_times": [],
        "errors": 0
    }

api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", None)
if not api_key:
    st.error("""
    ğŸš¨ **OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤**
    
    `.env` íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ ì¶”ê°€í•´ì£¼ì„¸ìš”:
    ```
    OPENAI_API_KEY=your_api_key_here
    ```
    
    ë˜ëŠ” Streamlit Cloudì—ì„œ Secretsë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
    """)
    st.stop()

# LLM ì„¤ì •
try:
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key)
    # API í‚¤ ìœ íš¨ì„±
    test_response = llm.invoke("test")
except Exception as e:
    st.error(f"ğŸš¨ OpenAI API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    if "api_key" in str(e).lower():
        st.error("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# UI í—¤ë”
st.title("ğŸ’¹ ê¸ˆìœµ ìƒë‹´ ì±—ë´‡")
st.markdown("ê¸ˆìœµì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# í•œê¸€ í°íŠ¸ ë“±ë¡
font_path = os.path.join(os.path.dirname(__file__), "NanumGothic-Regular.ttf")
pdfmetrics.registerFont(TTFont("NanumGothic", font_path))

# CSS
st.markdown(
    """
    <style>
    .chat-input textarea { height: 3em !important; border-radius: 8px; padding: 10px; font-size: 1rem; }
    .send-button { background-color: #1E90FF; color: white; border-radius: 6px; padding: 0.5em 1.2em; border: none; cursor: pointer; margin-left: 10px; }
    .chat-row { display: flex; align-items: center; gap: 10px; }
    </style>
    """,
    unsafe_allow_html=True,
)

# ====== ì„¸ì…˜ ======
if "messages" not in st.session_state:
    st.session_state.messages = []

# ====== ì—…ë¡œë“œ ìœ„ì ¯ ======
uploaded_files = st.file_uploader("PDF ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True)
st.markdown("##### ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

# ====== ë²¡í„°ìŠ¤í† ì–´ ë¹Œë” (ë©€í‹° PDF, ê²¬ê³ í™”) ======
@st.cache_resource(show_spinner=False)
def build_vs_multi(
    files: tuple[bytes, ...],
    file_names: tuple[str, ...],
    chunk_size: int = 800,
    chunk_overlap: int = 200,
    embed_model: str = "text-embedding-3-small",
):
    # ìºì‹œ í‚¤ ì•ˆì •í™”: íŒŒì¼ í•´ì‹œ + íŒŒë¼ë¯¸í„°
    _ = (chunk_size, chunk_overlap, embed_model, tuple(hashlib.md5(b).hexdigest() for b in files))

    all_docs = []
    tmp_paths = []
    processed_files = []  # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡
    
    try:
        for idx, (raw, fname) in enumerate(zip(files, file_names)):
            if not raw:
                st.warning(f"âš ï¸ '{fname}': íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                continue
                
            h = hashlib.md5(raw).hexdigest()
            path = f"/tmp/{h}.pdf"
            
            try:
                with open(path, "wb") as f:
                    f.write(raw)
                tmp_paths.append(path)

                # PDF ë¡œë“œ (ì•”í˜¸í™”/ê¹¨ì§ ì˜ˆì™¸ ì²˜ë¦¬)
                pages = PyPDFLoader(path).load()
                if not pages:
                    st.warning(f"âš ï¸ '{fname}': ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue
                    
                total_text = "".join(page.page_content for page in pages)
                if len(total_text.strip()) < 30:
                    st.warning(f"âš ï¸ '{fname}': í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë µìŠµë‹ˆë‹¤.")
                    continue

                # ì²­í¬ ë¶„í• 
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size, 
                    chunk_overlap=chunk_overlap
                )
                docs = splitter.split_documents(pages)
                
                # ê° ì²­í¬ì— íŒŒì¼ ì •ë³´ ì¶”ê°€
                for d in docs:
                    d.metadata = {
                        **d.metadata, 
                        "source_file": fname,
                        "file_index": idx,  # íŒŒì¼ ìˆœì„œ ì¶”ê°€
                        "total_files": len(files)  # ì „ì²´ íŒŒì¼ ìˆ˜ ì¶”ê°€
                    }
                
                all_docs.extend(docs)
                processed_files.append(fname)
                st.success(f"âœ… '{fname}' ì²˜ë¦¬ ì™„ë£Œ ({len(docs)}ê°œ ì²­í¬)")

            except Exception as e:
                error_msg = str(e).lower()
                if "encrypted" in error_msg or "password" in error_msg:
                    st.warning(f"ğŸ”’ '{fname}': ì•”í˜¸í™”ëœ PDFì…ë‹ˆë‹¤. ì•”í˜¸ë¥¼ í•´ì œí•˜ê³  ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                elif "damaged" in error_msg or "corrupted" in error_msg:
                    st.warning(f"ğŸ’¥ '{fname}': ì†ìƒëœ íŒŒì¼ì…ë‹ˆë‹¤.")
                elif "permission" in error_msg:
                    st.warning(f"ğŸš« '{fname}': íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning(f"âŒ '{fname}' ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue

        if not all_docs:
            raise ValueError("ìœ íš¨í•œ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        st.info(f"ğŸ“„ ì´ {len(processed_files)}ê°œ íŒŒì¼ì—ì„œ {len(all_docs)}ê°œ ì²­í¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ë³„ ì²­í¬ ìˆ˜ í‘œì‹œ
        file_chunk_counts = {}
        for doc in all_docs:
            fname = doc.metadata.get("source_file", "Unknown")
            file_chunk_counts[fname] = file_chunk_counts.get(fname, 0) + 1
        
        for fname, count in file_chunk_counts.items():
            st.write(f"  â€¢ {fname}: {count}ê°œ ì²­í¬")

        try:
            vs = FAISS.from_documents(all_docs, OpenAIEmbeddings(model=embed_model))
            st.success(f"âœ… {len(all_docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ğŸš¨ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            if "api" in str(e).lower():
                st.error("OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API í‚¤ì™€ ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise e
        
        return vs, all_docs

    finally:
        # ì„ì‹œíŒŒì¼ ì •ë¦¬
        for p in tmp_paths:
            try:
                os.remove(p)
            except Exception:
                pass

    #show_file_processing_status(processed_files, len(files))

# ====== í”„ë¡¬í”„íŠ¸ ======
stuff_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë„ˆëŠ” í•œêµ­ì–´ ê¸ˆìœµ ì „ë¬¸ê°€ì•¼. ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ì•„ ë‹µí•´.\n"
        "- í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ\n"
        "- ì´ˆë³´ìë„ ì´í•´ ê°€ëŠ¥í•˜ê²Œ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…\n"
        "- ìˆ˜ì¹˜/ì „ëµ/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ(ê¸°ê°„, ì¡°ê±´ í¬í•¨)\n"
        "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ëª¨ë¥¸ë‹¤'ê³  ë‹µí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ\n"
        "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n"
        "**ë©€í‹° ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ìš” ê·œì¹™**:\n"
        "1. ì»¨í…ìŠ¤íŠ¸ì— ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ëª¨ë“  ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€\n"
        "2. ê° ë¬¸ì„œì˜ ê³ ìœ í•œ íŠ¹ì§•ì´ë‚˜ ì°¨ì´ì ì´ ìˆìœ¼ë©´ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì„¤ëª…\n"
        "3. ë¬¸ì„œê°„ ê³µí†µì ê³¼ ì°¨ì´ì ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ ì œì‹œ\n"
        "4. ë‹µë³€ ì‹œ 'ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œëŠ”...', 'ë‘ ë²ˆì§¸ ë¬¸ì„œì—ì„œëŠ”...' ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ ëª…ì‹œ\n"
        "5. ì§ˆë¬¸ì´ ì „ì²´ì ì¸ ë¹„êµë‚˜ ì¢…í•©ì„ ìš”êµ¬í•˜ë©´ ëª¨ë“  ë¬¸ì„œë¥¼ ì•„ìš°ë¥´ëŠ” ë‹µë³€ ì œê³µ\n\n"
        "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    ),
)

# ====== ìš”ì•½ í•¨ìˆ˜ ======
@st.cache_data(show_spinner=False)
def summarize_once(_docs, _hash):
    _ = str(_hash)
    map_prompt = PromptTemplate(
        input_variables=["text"],
        template=(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ì„ í•œêµ­ì–´ë¡œ 3~5ê°œ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½í•´.\n"
            "- ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ\n"
            "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n{text}\n\nìš”ì•½:"
        ),
    )
    combine_prompt = PromptTemplate(
        input_variables=["text"],
        template=(
            "ì•„ë˜ ìš”ì•½ë“¤ì„ í•œêµ­ì–´ ë‹¨ë½ 3~5ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•´. ì¤‘ë³µì€ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¨.\n\n{text}\n\nìµœì¢… ìš”ì•½:"
        ),
    )
    chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=map_prompt, combine_prompt=combine_prompt)
    return chain.run(_docs)

# ====== ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ =======
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
    def on_llm_start(self, *args, **kwargs):
        self.text = ""
        if self.container:
            self.container.markdown("")
    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        if self.container:
            self.container.markdown(self.text)
    def on_llm_end(self, *args, **kwargs):
        pass

def log_async(q, a):
        """ë¹„ë™ê¸°ë¡œ ëŒ€í™” ë¡œê·¸ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        def _w():
            try:
                conn = sqlite3.connect("chat_logs.db", check_same_thread=False)
                cur = conn.cursor()
                cur.execute(
                    """
                    CREATE TABLE IF NOT EXISTS chat_logs (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        question TEXT, 
                        answer TEXT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                    """
                )
                cur.execute("INSERT INTO chat_logs (question, answer) VALUES (?,?)", (q, a))
                conn.commit()
                conn.close()
            except Exception as e:
                print(f"Log save error: {e}")
        
        threading.Thread(target=_w, daemon=True).start()


# ====== ê³¼ê±° ë©”ì‹œì§€ ë Œë” ======
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# í† ê¸€
with st.sidebar:
    st.markdown("---")
    st.subheader("âš™ï¸ ì¶”ê°€ ê¸°ëŠ¥")
    
    # ìš”ì•½/ì´ë¯¸ì§€ ìƒì„± í† ê¸€
    do_post = st.toggle(
        "ğŸ“Š ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œ ìë™ ìƒì„±", 
        key="do_postprocess", 
        value=False,
        help="PDF ë¬¸ì„œê°€ ì—…ë¡œë“œëœ ìƒíƒœì—ì„œ ë‹µë³€ ì‹œ ìë™ìœ¼ë¡œ ë¬¸ì„œ ìš”ì•½ê³¼ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í† ê¸€
    stream_on = st.toggle(
        "âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°", 
        key="do_stream", 
        value=True,
        help="ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ë„ë©´ ì™„ì„±ëœ ë‹µë³€ì„ í•œ ë²ˆì— ë³´ì—¬ì¤ë‹ˆë‹¤."
    )
    
    # í† ê¸€ ìƒíƒœ í‘œì‹œ
    if do_post:
        if st.session_state.get("last_docs"):
            st.success("âœ… ë‹¤ìŒ ë‹µë³€ë¶€í„° ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤")
        else:
            st.info("â„¹ï¸ PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ë©´ ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤")

# ====== ì›Œë“œí´ë¼ìš°ë“œ ======
def debug_search_results(sources, question):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë©€í‹° íŒŒì¼ ì²˜ë¦¬ ìƒíƒœë¥¼ í™•ì¸"""
    if not sources:
        st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    file_counts = {}
    total_chars = 0
    
    for doc in sources:
        source_file = doc.metadata.get("source_file", "Unknown")
        file_counts[source_file] = file_counts.get(source_file, 0) + 1
        total_chars += len(doc.page_content)
    
    with st.expander("ğŸ” ê²€ìƒ‰ ë””ë²„ê·¸ ì •ë³´", expanded=False):
        st.write(f"â€¢ ì´ ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {len(sources)}")
        st.write(f"â€¢ ì´ í…ìŠ¤íŠ¸ ê¸¸ì´: {total_chars:,} ë¬¸ì")
        
        for file_name, count in file_counts.items():
            percentage = (count / len(sources)) * 100
            st.write(f"â€¢ {file_name}: {count}ê°œ ì²­í¬ ({percentage:.1f}%)")
        
        # íŒŒì¼ë³„ ê· í˜• ê²€ì‚¬
        if len(file_counts) > 1:
            counts = list(file_counts.values())
            if max(counts) / min(counts) > 3:
                st.warning("âš ï¸ íŒŒì¼ê°„ ê²€ìƒ‰ ê²°ê³¼ ë¶ˆê· í˜• ê°ì§€!")

@st.cache_data(show_spinner=False)
def generate_wordcloud_image_cached(text: str, _hash: str):
    """ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ìƒì„± (ìºì‹œë¨)"""
    _ = str(_hash)
    
    # í•œê¸€ ë¶ˆìš©ì–´ í™•ì¥
    korean_stopwords = {
        "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë˜í•œ", "ë”°ë¼ì„œ", "ê·¸ë˜ì„œ", "ì¦‰", "ì˜ˆë¥¼ ë“¤ì–´",
        "ìˆìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ê²ƒì…ë‹ˆë‹¤", "ìˆ˜", "ë•Œ", "ë“±", "í†µí•´",
        "ëŒ€í•œ", "ìœ„í•œ", "ê´€ë ¨", "ê²½ìš°", "ë°©ë²•", "ì´ëŸ°", "ê·¸ëŸ°", "ì´ì™€", "ê°™ì€"
    }
    
    stop = set(STOPWORDS) | {"https", "http", "www", "com"} | korean_stopwords

    try:
        wc = WordCloud(
            font_path=font_path, 
            width=800, 
            height=400, 
            background_color="white", 
            collocations=False, 
            stopwords=stop,
            max_words=100,
            colormap='viridis'
        ).generate(text)
        
        buf = BytesIO()
        wc.to_image().save(buf, format="PNG")
        buf.seek(0)
        return buf
        
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# ====== ë©”ì¸ ì…ë ¥ ======
if question := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    # í™”ë©´ ìë¦¬ + ì½œë°±
    live_area = st.empty()
    handler = StreamHandler(live_area) if st.session_state.get("do_stream", False) else None
    cfg = {"callbacks": [handler]} if handler else {}

    st.session_state["last_sources"] = []
    pdf_mode, docs, retriever, base_retriever = False, None, None, None

    if uploaded_files:
        raws = tuple(f.getvalue() for f in uploaded_files if f is not None)
        names = tuple(f.name for f in uploaded_files if f is not None)
        combined_hash = hashlib.md5(b"".join(raws)).hexdigest() if raws else "nohash"

        pdf_mode, docs, retriever, base_retriever = False, None, None, None

        try:
            with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                vectorstore, docs = build_vs_multi(
                    raws, names, 
                    chunk_size=1000, 
                    chunk_overlap=150, 
                    embed_model="text-embedding-3-small"
                )
        except ValueError as e:
            st.error(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            st.info("ğŸ’¡ ë‹¤ë¥¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDFì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            pdf_mode = False
        except Exception as e:
            st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.info("ğŸ’¡ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            pdf_mode = False
        else:
            # Dense(ì„ë² ë”©) ê²€ìƒ‰ ì„¤ì •
            dense = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 12, "fetch_k": 40, "lambda_mult": 0.3})
            
            # BM25 ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì¡°ê±´ë¶€ ì‚¬ìš©
            try:
                from langchain_community.retrievers import BM25Retriever
                bm25 = BM25Retriever.from_documents(docs)
                ensemble = EnsembleRetriever(retrievers=[bm25, dense], weights=[0.4, 0.6])
                st.success("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™”")
            except ImportError:
                # BM25 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ëŠ” ê²½ìš°
                ensemble = dense
                st.info("ğŸ” Dense ê²€ìƒ‰ í™œì„±í™” (BM25 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜)")
            except Exception as e:
                # BM25 ì´ˆê¸°í™” ì‹¤íŒ¨í•œ ê²½ìš°
                ensemble = dense
                st.info("ğŸ” Dense ê²€ìƒ‰ í™œì„±í™” (BM25 ì´ˆê¸°í™” ì‹¤íŒ¨)")

            compressor = LLMChainExtractor.from_llm(llm)
            retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble)

            base_retriever = dense
            pdf_mode = True
            st.session_state["last_file_hash"] = combined_hash
            st.markdown(
                """
                <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                    <strong>PDFë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</strong>
                </div>
                """,
                unsafe_allow_html=True,
            )
    else:
        st.markdown(
            """
            <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                <strong>PDF ì—†ì´ ê¸°ë³¸ì ì¸ ê¸ˆìœµ ì •ë³´ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.</strong>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.session_state.pop("file_meta", None)


    # ====== ë‹µë³€ ìƒì„± ======
    if pdf_mode and retriever:
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": stuff_prompt},
            return_source_documents=True,
        )

        response = ""
        sources = []
        try:
            with st.spinner("ğŸ” ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
                res = measure_response_time(
                    lambda: qa_chain.invoke({"query": question}, config=cfg)
                )
                response = (res.get("result") or "").strip()
                sources = res.get("source_documents", []) or []
                
                # ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹… (ê°œë°œìš©)
                if st.session_state.get("debug_mode", False):
                    debug_search_results(sources, question)
                
        except Exception as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg:
                st.error("â³ API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                response = "ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            elif "timeout" in error_msg:
                st.error("â±ï¸ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ í•´ë³´ì„¸ìš”.")
                response = "ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ í•´ì£¼ì‹œê² ì–´ìš”?"
            elif "token" in error_msg:
                st.error("ğŸ“ ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ì–´ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì‘ì€ PDFë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
                response = "ë¬¸ì„œê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì‘ì€ ë¬¸ì„œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                st.error(f"ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            sources = []

        # ====== ë Œë” & ì„¸ì…˜ ì €ì¥ ======
        st.session_state["last_response"] = response
        if pdf_mode and docs:
            st.session_state["last_docs"] = docs
            st.session_state["last_file_hash"] = combined_hash
        else:
            st.session_state.pop("last_docs", None)

        try:
            if handler:
                live_area.empty()
        except Exception:
            pass


        # ì†ŒìŠ¤ ì „ì²˜ë¦¬ í›„ ì„¸ì…˜ ì €ì¥
        processed_sources = []
        _seen = set()
        file_sources = {}

        for d in sources:
            src_name = d.metadata.get("source_file") or os.path.basename(d.metadata.get("source", "?"))
            pg = d.metadata.get("page")
            pg_num = (pg + 1) if isinstance(pg, int) else "?"

            # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
            if src_name not in file_sources:
                file_sources[src_name] = []

            key = (src_name, pg_num)
            if key in _seen:
                continue
            _seen.add(key)

            snippet = (d.page_content or "").strip()
            if len(snippet) > 400:
                snippet = snippet[:400] + "â€¦"
            
            file_sources[src_name].append({"page": pg_num, "snippet": snippet})

        # íŒŒì¼ë³„ë¡œ ì •ë¦¬ëœ ì†ŒìŠ¤ ì •ë³´ ìƒì„±
        for fname, pages in file_sources.items():
            for page_info in pages:
                processed_sources.append({
                    "file": fname, 
                    "page": page_info["page"], 
                    "snippet": page_info["snippet"]
                })

        st.session_state["last_sources"] = processed_sources

        # ë‹µë³€ í‘œì‹œ ë¶€ë¶„ì—ì„œ íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ ë³´ì—¬ì£¼ê¸°:
        if st.session_state.get("last_sources"):
            with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
                # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ í‘œì‹œ
                current_sources = st.session_state["last_sources"]
                file_groups = {}
                
                for source in current_sources:
                    fname = source["file"]
                    if fname not in file_groups:
                        file_groups[fname] = []
                    file_groups[fname].append(source)
                
                for fname, sources_in_file in file_groups.items():
                    st.markdown(f"**ğŸ“ {fname}**")
                    for i, s in enumerate(sources_in_file, 1):
                        st.markdown(f"  {i}. p.{s['page']}")
                        st.code(s["snippet"], language="text")
                    st.markdown("---")

        # ì²« ë²ˆì§¸ ê²€ìƒ‰ì—ì„œ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš° ë°±ì—… ê²€ìƒ‰
        if not response or len(response.strip()) < 50 or "ëª¨ë¥¸ë‹¤" in response:
            st.info("ğŸ” ì¶”ê°€ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # ë°±ì—… ê²€ìƒ‰ ì²´ì¸ ìƒì„±
            qa_chain_loose = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff", 
                retriever=base_retriever,
                chain_type_kwargs={"prompt": stuff_prompt},
                return_source_documents=True,
            )
            
            # ë‹¨ìˆœí™”ëœ ë°±ì—… ê²€ìƒ‰
            backup_queries = [
                f"í•µì‹¬ í‚¤ì›Œë“œ: {question}",
                f"ê°„ë‹¨í•œ ì§ˆë¬¸: {question}",
                question  # ì›ë³¸ ì§ˆë¬¸
            ]

            for backup_query in backup_queries:
                try:
                    backup_result = qa_chain_loose.invoke({"query": backup_query})
                    backup_response = (backup_result.get("result") or "").strip()
                    
                    if backup_response and "ëª¨ë¥¸ë‹¤" not in backup_response and len(backup_response) > 50:
                        response = backup_response
                        if backup_query != question:
                            st.info(f"ğŸ’¡ '{backup_query}'ë¡œ ì¬ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                        break
                except:
                    continue

            # ì—¬ì „íˆ ë‹µë³€ì´ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë§ˆì§€ë§‰ ì‹œë„
            if not response or len(response.strip()) < 20:
                try:
                    keyword_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 3ê°œë§Œ ì¶”ì¶œí•´ì¤˜ (ì‰¼í‘œë¡œ êµ¬ë¶„): {question}"
                    keyword_result = llm.invoke(keyword_prompt)
                    keywords = getattr(keyword_result, "content", "").strip()
                    
                    if keywords:
                        result3 = qa_chain_loose.invoke({"query": keywords}, config=cfg)
                        test_response = (result3.get("result") or "").strip()
                        
                        if test_response and "ëª¨ë¥¸ë‹¤" not in test_response:
                            response = f"í‚¤ì›Œë“œ '{keywords}' ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼:\n\n{test_response}"
                            st.info(f"ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰: '{keywords}'")
                            
                except Exception:
                    pass

        qa_chain_loose = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=base_retriever if base_retriever else retriever,
            chain_type_kwargs={"prompt": stuff_prompt},
            return_source_documents=True,
        )
        try:
            result2 = qa_chain_loose.invoke({"query": rephrased or question}, config=cfg)
            response = (result2.get("result") or "").strip()
        except Exception:
            response = ""

        if not response:
            try:
                # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                keyword_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ 3ê°œë§Œ ì¶”ì¶œí•´ì¤˜ (ì‰¼í‘œë¡œ êµ¬ë¶„): {question}"
                keyword_result = llm.invoke(keyword_prompt)
                keywords = getattr(keyword_result, "content", "").strip()
                
                # í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰
                if keywords:
                    result3 = qa_chain_loose.invoke({"query": keywords}, config=cfg)
                    test_response = (result3.get("result") or "").strip()
                    
                    if test_response and "ëª¨ë¥¸ë‹¤" not in test_response:
                        response = f"í‚¤ì›Œë“œ '{keywords}' ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼:\n\n{test_response}"
                        
            except Exception:
                pass
    else:
        prompt = PromptTemplate.from_template(
            template=(
                "ë„ˆëŠ” í•œêµ­ì–´ ê¸ˆìœµ ì „ë¬¸ê°€ì•¼. ì¼ë°˜ì ì¸ ê¸ˆìœµ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\n"
                "- í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ\n"
                "- ì´ˆë³´ìë„ ì´í•´ ê°€ëŠ¥í•˜ê²Œ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…\n"
                "- ìˆ˜ì¹˜/ì „ëµ/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ(ê¸°ê°„, ì¡°ê±´ í¬í•¨)\n"
                "- ìœ ìš©í•œ ì¼ë°˜ì  ì •ë³´ë¥¼ í¬í•¨í•´ë„ ë¨\n\n"
                "ì§ˆë¬¸: {question}\n"
                "ë‹µë³€:"
            )
        )

        try:
            with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                r = llm.invoke(prompt.format(context="", question=question), config=cfg)
                response = getattr(r, "content", str(r))
        except Exception as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg:
                response = "â³ ìš”ì²­ì´ ë§ì•„ ì ì‹œ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. 1ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                st.warning("API ìš”ì²­ í•œë„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            elif "api_key" in error_msg:
                response = "ğŸ”’ API í‚¤ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                st.error("API í‚¤ ì˜¤ë¥˜ ë°œìƒ")
            else:
                response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                st.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")

    
    # ====== ë‹µë³€ í‘œì‹œ ======
    with st.chat_message("assistant"):
        st.markdown(response)

        # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ (íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”)
        if st.session_state.get("last_sources"):
            with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
                current_sources = st.session_state["last_sources"]
                file_groups = {}
                
                # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
                for source in current_sources:
                    fname = source["file"]
                    if fname not in file_groups:
                        file_groups[fname] = []
                    file_groups[fname].append(source)
                
                # íŒŒì¼ë³„ë¡œ í‘œì‹œ
                for idx, (fname, sources_in_file) in enumerate(file_groups.items()):
                    st.markdown(f"**ğŸ“ {fname}**")
                    for i, s in enumerate(sources_in_file, 1):
                        st.markdown(f"  {i}. p.{s['page']}")
                        st.code(s["snippet"], language="text")

                    if idx < len(file_groups) - 1:
                        st.markdown("---")

        # ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œ ê¸°ëŠ¥
        if st.session_state.get("do_postprocess"):
            if st.session_state.get("last_docs"):
                st.markdown("---")
                st.subheader("ğŸ“Š ë¬¸ì„œ ë¶„ì„")
                
                try:
                    with st.spinner("ğŸ“ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        summary = summarize_once(
                            st.session_state["last_docs"], 
                            st.session_state.get("last_file_hash", "nohash")
                        )
                    
                    with st.expander("ğŸ“‹ ë¬¸ì„œ ìš”ì•½", expanded=True):
                        st.info(summary)
                    
                    try:
                        with st.spinner("ğŸ¨ í‚¤ì›Œë“œë¥¼ ì‹œê°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            hash_key = st.session_state.get("last_file_hash", "nohash")
                            wordcloud_img = generate_wordcloud_image_cached(summary, hash_key)
                        
                        if wordcloud_img:
                            with st.expander("â˜ï¸ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ", expanded=True):
                                st.image(wordcloud_img, use_container_width=True)
                        
                    except Exception as e:
                        st.warning(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                        
                except Exception as e:
                    st.error(f"ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            elif len(response) > 100:
                st.markdown("---")
                st.subheader("ğŸ’¡ ë‹µë³€ í‚¤ì›Œë“œ")
                
                try:
                    simple_wordcloud = generate_wordcloud_image_cached(response, "response_based")
                    if simple_wordcloud:
                        with st.expander("â˜ï¸ ë‹µë³€ í‚¤ì›Œë“œ", expanded=False):
                            st.image(simple_wordcloud, use_container_width=True)
                            st.caption("í˜„ì¬ ë‹µë³€ì—ì„œ ì¶”ì¶œí•œ ì£¼ìš” í‚¤ì›Œë“œë“¤ì…ë‹ˆë‹¤.")
                
                except Exception as e:
                    st.warning(f"í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        st.session_state.messages.append({"role": "assistant", "content": response})

    # ë¹„ë™ê¸° ë¡œê·¸ ì €ì¥
    log_async(question, response)

# ====== ì‚¬ì´ë“œë°” ======
with st.sidebar:
    st.markdown("## ğŸ“œ íˆ¬ì FAQ & ê°€ì´ë“œ")
    with st.expander("ğŸ“Œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸"):
        st.markdown(
            """
            **1. ETFë€?**  
            ìƒì¥ì§€ìˆ˜í€ë“œë¡œ, ì§€ìˆ˜ë¥¼ ì¶”ì¢…í•˜ëŠ” í€ë“œì…ë‹ˆë‹¤. ì£¼ì‹ì²˜ëŸ¼ ê±°ë˜ë©ë‹ˆë‹¤.

            **2. ETFì™€ í€ë“œ ì°¨ì´?**  
            í€ë“œëŠ” í•˜ë£¨ 1ë²ˆ ê±°ë˜, ETFëŠ” ì‹¤ì‹œê°„ ê±°ë˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

            **3. ì†Œì•¡ìœ¼ë¡œë„ ê°€ëŠ¥í•œê°€ìš”?**  
            ë„¤. ETFëŠ” 1ì£¼ ë‹¨ìœ„ë¡œ ë§¤ìˆ˜í•  ìˆ˜ ìˆì–´, ìˆ˜ì²œ ì›ìœ¼ë¡œë„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            **4. ìˆ˜ìµë¥ ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?**  
            (í˜„ì¬ê°€ - ë§¤ìˆ˜ê°€) / ë§¤ìˆ˜ê°€ Ã— 100
            """
        )
    with st.expander("ğŸ’¡ ì´ˆë³´ìë¥¼ ìœ„í•œ íˆ¬ì ê°œë…"):
        st.markdown(
            """
            **ğŸ“ ë¶„ì‚° íˆ¬ì**  
            ì—¬ëŸ¬ ìì‚°ì— ë‚˜ëˆ„ì–´ íˆ¬ìí•¨ìœ¼ë¡œì¨ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì´ëŠ” ì „ëµì…ë‹ˆë‹¤.

            **ğŸ“ ë¦¬ìŠ¤í¬ë€?**  
            ìˆ˜ìµì´ ë“¤ì­‰ë‚ ì­‰í•˜ê±°ë‚˜ ì†ì‹¤ì´ ë‚  ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

            **ğŸ“ ETF ì¥ì **  
            - ë¶„ì‚° íˆ¬ì íš¨ê³¼  
            - ë‚®ì€ ìˆ˜ìˆ˜ë£Œ  
            - ë‹¤ì–‘í•œ ìì‚°êµ°(ì£¼ì‹, ì±„ê¶Œ, ê¸ˆ ë“±)

            **ğŸ“ ì¥ê¸° íˆ¬ìë€?**  
            ë‹¨ê¸° ì‹œì„¸ ë³€ë™ì— í”ë“¤ë¦¬ì§€ ì•Šê³  ì¼ì • ê¸°ê°„ ì´ìƒ ë³´ìœ í•˜ì—¬ ìˆ˜ìµì„ ê¸°ëŒ€í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.
            """
        )

# can_export ë³€ìˆ˜ ì •ì˜
can_export = bool(st.session_state.get("last_response"))

with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ”„ ëŒ€í™” ê´€ë¦¬")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì‚­ì œ", use_container_width=True):
            st.session_state.messages = []
            st.session_state.pop("last_response", None)
            st.session_state.pop("last_sources", None)
            st.rerun()
    
    with col2:
        if st.button("ğŸ’¾ ëŒ€í™” ì €ì¥", use_container_width=True):
            if st.session_state.messages:
                # ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                conversation = ""
                for msg in st.session_state.messages:
                    role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì±—ë´‡"
                    conversation += f"{role}: {msg['content']}\n\n"
                
                st.download_button(
                    "ğŸ“„ ëŒ€í™” ë‚´ì—­ ë‹¤ìš´ë¡œë“œ",
                    data=conversation.encode('utf-8'),
                    file_name=f"ëŒ€í™”ê¸°ë¡_{time.strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    key="download_chat"
                )

# ì„±ëŠ¥ ìœ„ì ¯
with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ”§ ê°œë°œì ë„êµ¬")
    
    debug_mode = st.toggle(
        "ğŸ› ê²€ìƒ‰ ë””ë²„ê·¸", 
        key="debug_mode", 
        value=False,
        help="ê²€ìƒ‰ ê²°ê³¼ì˜ íŒŒì¼ë³„ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
    )

with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥")
    
    metrics = st.session_state.performance_metrics
    
    if metrics["query_count"] > 0:
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric(
                "ì´ ì¿¼ë¦¬", 
                metrics["query_count"],
                help="ì´ë²ˆ ì„¸ì…˜ì—ì„œ ì²˜ë¦¬í•œ ì´ ì§ˆë¬¸ ìˆ˜"
            )
            
        with col2:
            avg_time = metrics["total_time"] / metrics["query_count"]
            st.metric(
                "í‰ê·  ì‘ë‹µì‹œê°„", 
                f"{avg_time:.1f}ì´ˆ",
                help="í‰ê· ì ì¸ ì‘ë‹µ ìƒì„± ì‹œê°„"
            )
        
        # ì„±ê³µë¥  í‘œì‹œ
        success_rate = ((metrics["query_count"] - metrics["errors"]) / metrics["query_count"]) * 100
        st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        
        # ìµœê·¼ ì‘ë‹µì‹œê°„ ì°¨íŠ¸ (ê°„ë‹¨í•œ ë¼ì¸)
        if len(metrics["response_times"]) > 1:
            recent_times = [r["time"] for r in metrics["response_times"][-10:]]
            st.line_chart(recent_times)
            st.caption("ìµœê·¼ 10ê°œ ì‘ë‹µì‹œê°„ ì¶”ì´")
    else:
        st.info("ì•„ì§ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ====== í¬ìŠ¤íŠ¸í”„ë¡œì„¸ì‹± ìœ„ì ¯ ======

# ====== PDF ì €ì¥ ======
if st.button("ğŸ¤– ë‹µë³€ì„ PDFë¡œ ì €ì¥", disabled=not can_export, use_container_width=True):
    try:
        text = st.session_state.get("last_response", "")
        buf = BytesIO()
        doc = SimpleDocTemplate(
            buf,
            pagesize=A4,
            leftMargin=36,
            rightMargin=36,
            topMargin=36,
            bottomMargin=36,
        )
        styles = getSampleStyleSheet()
        kstyle = ParagraphStyle(
            "Korean",
            parent=styles["Normal"],
            fontName="NanumGothic",
            fontSize=12,
            leading=16,
            wordWrap="CJK",
        )
        text = text.replace("\n", "<br/>")
        story = [Paragraph(text, kstyle)]
        doc.build(story)
        st.session_state["pdf_download"] = buf.getvalue()
        st.toast("PDF ì¤€ë¹„ ì™„ë£Œ âœ…")
    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

pdf_bytes = st.session_state.get("pdf_download")
if pdf_bytes:
    st.download_button(
        "ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ",
        data=pdf_bytes,
        file_name="etf_response.pdf",
        mime="application/pdf",
        use_container_width=True,
        type="primary",
    )
