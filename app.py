import hashlib
import os
import sqlite3
from io import BytesIO

import streamlit as st
from dotenv import load_dotenv

# ====== LLM & LangChain (LangChain 0.3.27) ======
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever

# ====== PDF/ì‹œê°í™” ======
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.pagesizes import A4
from wordcloud import WordCloud, STOPWORDS

# ====== ê¸°ë³¸ ì„¤ì • ======
load_dotenv()

st.set_page_config(page_title="ETF ì±—ë´‡", page_icon="ğŸ’¹")

api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", None)
if not api_key:
    st.stop()  # API í‚¤ ì—†ìœ¼ë©´ ì¤‘ë‹¨

# LLM ì„¤ì •
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# UI í—¤ë”
st.title("ğŸ’¹ ê¸ˆìœµ ìƒë‹´ ì±—ë´‡")
st.markdown("ê¸ˆìœµì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# í•œê¸€ í°íŠ¸ ë“±ë¡
font_path = os.path.join(os.path.dirname(__file__), "NanumGothic-Regular.ttf")
pdfmetrics.registerFont(TTFont("NanumGothic", font_path))

# ê°„ë‹¨ CSS
st.markdown(
    """
    <style>
    .chat-input textarea { height: 3em !important; border-radius: 8px; padding: 10px; font-size: 1rem; }
    .send-button { background-color: #1E90FF; color: white; border-radius: 6px; padding: 0.5em 1.2em; border: none; cursor: pointer; margin-left: 10px; }
    .chat-row { display: flex; align-items: center; gap: 10px; }
    </style>
    """,
    unsafe_allow_html=True,
)

# ====== ì„¸ì…˜ ======
if "messages" not in st.session_state:
    st.session_state.messages = []

# ====== ì—…ë¡œë“œ ìœ„ì ¯ ======
uploaded_files = st.file_uploader("PDF ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True)
st.markdown("##### ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

# ====== ë²¡í„°ìŠ¤í† ì–´ ë¹Œë” (ë©€í‹° PDF, ê²¬ê³ í™”) ======
@st.cache_resource(show_spinner=False)
def build_vs_multi(
    files: tuple[bytes, ...],
    file_names: tuple[str, ...],
    chunk_size: int = 1000,
    chunk_overlap: int = 150,
    embed_model: str = "text-embedding-3-small",
):
    # ìºì‹œ í‚¤ ì•ˆì •í™”: íŒŒì¼ í•´ì‹œ + íŒŒë¼ë¯¸í„°
    _ = (chunk_size, chunk_overlap, embed_model, tuple(hashlib.md5(b).hexdigest() for b in files))

    all_docs = []
    tmp_paths = []
    try:
        for raw, fname in zip(files, file_names):
            if not raw:
                continue
            h = hashlib.md5(raw).hexdigest()
            path = f"/tmp/{h}.pdf"
            with open(path, "wb") as f:
                f.write(raw)
            tmp_paths.append(path)

            # PDF ë¡œë“œ (ì•”í˜¸í™”/ê¹¨ì§ ì˜ˆì™¸ ì²˜ë¦¬)
            try:
                pages = PyPDFLoader(path).load()
            except Exception as e:
                st.warning(f"'{fname}' ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue

            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            docs = splitter.split_documents(pages)
            for d in docs:
                d.metadata = {**d.metadata, "source_file": fname}
            all_docs.extend(docs)

        if not all_docs:
            raise ValueError("ìœ íš¨í•œ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

        vs = FAISS.from_documents(all_docs, OpenAIEmbeddings(model=embed_model))
        return vs, all_docs
    finally:
        # ì„ì‹œíŒŒì¼ ì •ë¦¬
        for p in tmp_paths:
            try:
                os.remove(p)
            except Exception:
                pass

# ====== í”„ë¡¬í”„íŠ¸ ======
stuff_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë„ˆëŠ” í•œêµ­ì–´ ê¸ˆìœµ ì „ë¬¸ê°€ì•¼. ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ì•„ ë‹µí•´.\n"
        "- í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ\n"
        "- ì´ˆë³´ìë„ ì´í•´ ê°€ëŠ¥í•˜ê²Œ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…\n"
        "- ìˆ˜ì¹˜/ì „ëµ/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ(ê¸°ê°„, ì¡°ê±´ í¬í•¨)\n"
        "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ëª¨ë¥¸ë‹¤'ê³  ë‹µí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ\n"
        "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n"
        "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    ),
)

# ====== ìš”ì•½ í•¨ìˆ˜ ======
@st.cache_data(show_spinner=False)
def summarize_once(_docs, _hash):
    _ = str(_hash)
    map_prompt = PromptTemplate(
        input_variables=["text"],
        template=(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ì„ í•œêµ­ì–´ë¡œ 3~5ê°œ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½í•´.\n"
            "- ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ\n"
            "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n{text}\n\nìš”ì•½:"
        ),
    )
    combine_prompt = PromptTemplate(
        input_variables=["text"],
        template=(
            "ì•„ë˜ ìš”ì•½ë“¤ì„ í•œêµ­ì–´ ë‹¨ë½ 3~5ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•´. ì¤‘ë³µì€ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¨.\n\n{text}\n\nìµœì¢… ìš”ì•½:"
        ),
    )
    chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=map_prompt, combine_prompt=combine_prompt)
    return chain.run(_docs)

# ====== ê³¼ê±° ë©”ì‹œì§€ ë Œë” ======
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# í† ê¸€
do_post = st.toggle("ìš”ì•½/ì´ë¯¸ì§€ ìƒì„± ì¼œê¸°", key="do_postprocess", value=False)

# ====== ì›Œë“œí´ë¼ìš°ë“œ ======
@st.cache_data(show_spinner=False)
def generate_wordcloud_image_cached(text: str, _hash: str):
    _ = str(_hash)
    stop = set(STOPWORDS) | {"https", "http", "www", "com"}
    wc = WordCloud(font_path=font_path, width=800, height=400, background_color="white", collocations=False, stopwords=stop).generate(text)
    buf = BytesIO()
    wc.to_image().save(buf, format="PNG")
    buf.seek(0)
    return buf

# ====== ë©”ì¸ ì…ë ¥ ======
if question := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    pdf_mode, docs, retriever, base_retriever = False, None, None, None

    if uploaded_files:
        raws = tuple(f.getvalue() for f in uploaded_files if f is not None)
        names = tuple(f.name for f in uploaded_files if f is not None)
        combined_hash = hashlib.md5(b"".join(raws)).hexdigest() if raws else "nohash"

        try:
            vectorstore, docs = build_vs_multi(raws, names, chunk_size=1000, chunk_overlap=150, embed_model="text-embedding-3-small")
        except Exception as e:
            st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            pdf_mode = False
        else:
            base_retriever = vectorstore.as_retriever(search_kwargs={"k": 3, "fetch_k": 12}, search_type="mmr")
            compressor = LLMChainExtractor.from_llm(llm)
            retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)
            pdf_mode = True
            st.session_state["last_file_hash"] = combined_hash
            st.markdown(
                """
                <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                    <strong>PDFë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</strong>
                </div>
                """,
                unsafe_allow_html=True,
            )
    else:
        st.markdown(
            """
            <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                <strong>PDF ì—†ì´ ê¸°ë³¸ì ì¸ ê¸ˆìœµ ì •ë³´ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.</strong>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.session_state.pop("file_meta", None)

    # ====== ë‹µë³€ ìƒì„± ======
    if pdf_mode and retriever:
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": stuff_prompt},
            return_source_documents=False,
        )

        response = ""
        try:
            result = qa_chain.invoke({"query": question})
            response = (result.get("result") or "").strip()
        except Exception:
            response = ""

        if not response:
            try:
                r = llm.invoke(f"ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ìœ ë¦¬í•˜ê²Œ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬í‘œí˜„í•´ì¤˜: {question}")
                rephrased = getattr(r, "content", str(r)).strip()
            except Exception:
                rephrased = question

            qa_chain_loose = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=base_retriever if base_retriever else retriever,
                chain_type_kwargs={"prompt": stuff_prompt},
                return_source_documents=False,
            )
            try:
                result2 = qa_chain_loose.invoke({"query": rephrased or question})
                response = (result2.get("result") or "").strip()
            except Exception:
                response = ""

        if not response:
            try:
                summary = summarize_once(docs, st.session_state.get("last_file_hash", "nohash"))
                response = "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ë§¤ì¹­ì„ ì°¾ê¸° ì–´ë ¤ì›Œ ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤:\n\n" + summary
            except Exception:
                response = "ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ PDFë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    else:
        prompt = PromptTemplate.from_template(
            """
            ë„ˆëŠ” ê¸ˆìœµíˆ¬ì ë¶„ì•¼ì— íŠ¹í™”ëœ AIì•¼.
            ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ ê°„ê²°í•˜ê²Œ ë‹µí•´:
            1) ì‹ ë¢° ê°€ëŠ¥í•œ ì¶œì²˜ ê¸°ë°˜  2) ì´ˆë³´ì ìš©ì–´ í’€ì–´ì“°ê¸°  3) ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†Œ í¬í•¨
            ì§ˆë¬¸: {question}
            """
        )
        try:
            r = llm.invoke(prompt.format(question=question))
            response = getattr(r, "content", str(r))
        except Exception as e:
            response = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # ====== ë Œë” & ì„¸ì…˜ ì €ì¥ ======
    st.session_state["last_response"] = response
    st.session_state["last_docs"] = docs

    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})

    # ë¹„ë™ê¸° ë¡œê·¸ ì €ì¥
    import threading

    def log_async(q, a):
        def _w():
            conn = sqlite3.connect("chat_logs.db", check_same_thread=False)
            cur = conn.cursor()
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS chat_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    question TEXT, answer TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            cur.execute("INSERT INTO chat_logs (question, answer) VALUES (?,?)", (q, a))
            conn.commit()
            conn.close()
        threading.Thread(target=_w, daemon=True).start()

    log_async(question, response)

# ====== ì‚¬ì´ë“œë°” ======
with st.sidebar:
    st.markdown("## ğŸ“œ íˆ¬ì FAQ & ê°€ì´ë“œ")
    with st.expander("ğŸ“Œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸"):
        st.markdown(
            """
            **1. ETFë€?**  
            ìƒì¥ì§€ìˆ˜í€ë“œë¡œ, ì§€ìˆ˜ë¥¼ ì¶”ì¢…í•˜ëŠ” í€ë“œì…ë‹ˆë‹¤. ì£¼ì‹ì²˜ëŸ¼ ê±°ë˜ë©ë‹ˆë‹¤.

            **2. ETFì™€ í€ë“œ ì°¨ì´?**  
            í€ë“œëŠ” í•˜ë£¨ 1ë²ˆ ê±°ë˜, ETFëŠ” ì‹¤ì‹œê°„ ê±°ë˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

            **3. ì†Œì•¡ìœ¼ë¡œë„ ê°€ëŠ¥í•œê°€ìš”?**  
            ë„¤. ETFëŠ” 1ì£¼ ë‹¨ìœ„ë¡œ ë§¤ìˆ˜í•  ìˆ˜ ìˆì–´, ìˆ˜ì²œ ì›ìœ¼ë¡œë„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            **4. ìˆ˜ìµë¥ ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?**  
            (í˜„ì¬ê°€ - ë§¤ìˆ˜ê°€) / ë§¤ìˆ˜ê°€ Ã— 100
            """
        )
    with st.expander("ğŸ’¡ ì´ˆë³´ìë¥¼ ìœ„í•œ íˆ¬ì ê°œë…"):
        st.markdown(
            """
            **ğŸ“ ë¶„ì‚° íˆ¬ì**  
            ì—¬ëŸ¬ ìì‚°ì— ë‚˜ëˆ„ì–´ íˆ¬ìí•¨ìœ¼ë¡œì¨ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì´ëŠ” ì „ëµì…ë‹ˆë‹¤.

            **ğŸ“ ë¦¬ìŠ¤í¬ë€?**  
            ìˆ˜ìµì´ ë“¤ì­‰ë‚ ì­‰í•˜ê±°ë‚˜ ì†ì‹¤ì´ ë‚  ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

            **ğŸ“ ETF ì¥ì **  
            - ë¶„ì‚° íˆ¬ì íš¨ê³¼  
            - ë‚®ì€ ìˆ˜ìˆ˜ë£Œ  
            - ë‹¤ì–‘í•œ ìì‚°êµ°(ì£¼ì‹, ì±„ê¶Œ, ê¸ˆ ë“±)

            **ğŸ“ ì¥ê¸° íˆ¬ìë€?**  
            ë‹¨ê¸° ì‹œì„¸ ë³€ë™ì— í”ë“¤ë¦¬ì§€ ì•Šê³  ì¼ì • ê¸°ê°„ ì´ìƒ ë³´ìœ í•˜ì—¬ ìˆ˜ìµì„ ê¸°ëŒ€í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.
            """
        )

# ====== í¬ìŠ¤íŠ¸í”„ë¡œì„¸ì‹± ìœ„ì ¯ ======
can_export = bool(st.session_state.get("last_response"))

if st.session_state.get("do_postprocess") and st.session_state.get("last_docs"):
    summary = summarize_once(st.session_state["last_docs"], st.session_state.get("last_file_hash", "nohash"))
    st.success(summary)

    hash_key = st.session_state.get("last_file_hash", "nohash")
    st.image(generate_wordcloud_image_cached(summary, hash_key), use_container_width=True)

# ====== PDF ì €ì¥ ======
if st.button("ğŸ¤– ë‹µë³€ì„ PDFë¡œ ì €ì¥", disabled=not can_export, use_container_width=True):
    try:
        text = st.session_state.get("last_response", "")
        buf = BytesIO()
        doc = SimpleDocTemplate(
            buf,
            pagesize=A4,
            leftMargin=36,
            rightMargin=36,
            topMargin=36,
            bottomMargin=36,
        )
        styles = getSampleStyleSheet()
        kstyle = ParagraphStyle(
            "Korean",
            parent=styles["Normal"],
            fontName="NanumGothic",
            fontSize=12,
            leading=16,
            wordWrap="CJK",
        )
        text = text.replace("\n", "<br/>")
        story = [Paragraph(text, kstyle)]
        doc.build(story)
        st.session_state["pdf_download"] = buf.getvalue()
        st.toast("PDF ì¤€ë¹„ ì™„ë£Œ âœ…")
    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

pdf_bytes = st.session_state.get("pdf_download")
if pdf_bytes:
    st.download_button(
        "ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ",
        data=pdf_bytes,
        file_name="etf_response.pdf",
        mime="application/pdf",
        use_container_width=True,
        type="primary",
    )
