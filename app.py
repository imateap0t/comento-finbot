import base64
import hashlib
import streamlit as st
import sqlite3, textwrap
from dotenv import load_dotenv
import os
from io import BytesIO
from langchain.text_splitter import RecursiveCharacterTextSplitter
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import threading
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import ttfonts

from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") or st.secrets["OPENAI_API_KEY"]

# LLM ì„¤ì •
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# í•œê¸€ í°íŠ¸ ë“±ë¡
font_path = os.path.join(os.path.dirname(__file__), "NanumGothic-Regular.ttf")
pdfmetrics.registerFont(TTFont('NanumGothic', font_path))

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ETF ì±—ë´‡", page_icon="ğŸ’¹")
st.title("ğŸ’¹ ê¸ˆìœµ ìƒë‹´ ì±—ë´‡")
st.markdown("ê¸ˆìœµì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")
st.markdown("")
st.markdown(
    """
    <style>
    .chat-input textarea {
        height: 3em !important;
        border-radius: 8px;
        padding: 10px;
        font-size: 1rem;
    }
    .send-button {
        background-color: #1E90FF;
        color: white;
        border-radius: 6px;
        padding: 0.5em 1.2em;
        border: none;
        cursor: pointer;
        margin-left: 10px;
    }
    .chat-row {
        display: flex;
        align-items: center;
        gap: 10px;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ì—…ë¡œë“œ ë° ì…ë ¥
uploaded_files = st.file_uploader("PDF ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True)
st.markdown("##### ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

@st.cache_resource(show_spinner=False)
def build_vs_multi(files: tuple[bytes, ...]):
    all_docs = []
    for raw in files:
        h = hashlib.md5(raw).hexdigest()
        path = f"/tmp/{h}.pdf"
        with open(path, "wb") as f:
            f.write(raw)
        pages = PyPDFLoader(path).load()
        docs = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150).split_documents(pages)
        all_docs.extend(docs)
    vs = FAISS.from_documents(all_docs, OpenAIEmbeddings(model="text-embedding-3-small"))
    return vs, all_docs


# ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µ
stuff_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë„ˆëŠ” í•œêµ­ì–´ ê¸ˆìœµ ì „ë¬¸ê°€ë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê³  ì •í™•íˆ ë‹µí•´ì¤˜.\n"
        "- ì´ˆë³´ìë„ ì´í•´ ê°€ëŠ¥í•˜ê²Œ ì„¤ëª…\n"
        "- ìˆ˜ì¹˜/ì „ëµ/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ\n"
        "- ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê¸°\n\n"
        "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    ),
)


# --- ì´ˆê¸°í™” ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- ì—…ë¡œë“œ ì²˜ë¦¬ & ë²¡í„°ìŠ¤í† ì–´ ìºì‹œ ---
import hashlib, threading
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever

@st.cache_resource(show_spinner=False)
def build_vectorstore(file_bytes: bytes):
    file_hash = hashlib.md5(file_bytes).hexdigest()
    pdf_path = f"/tmp/{file_hash}.pdf"
    with open(pdf_path, "wb") as f:
        f.write(file_bytes)
    loader = PyPDFLoader(pdf_path)
    pages = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = splitter.split_documents(pages)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vs = FAISS.from_documents(docs, embeddings)
    return vs, file_hash, docs

# --- ê³¼ê±° ë©”ì‹œì§€ ë Œë” ---
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ë¯¸ë¦¬ UI í† ê¸€
do_post = st.toggle("ìš”ì•½/ì´ë¯¸ì§€ ìƒì„± ì¼œê¸°", key="do_postprocess", value=False)

# --- ì…ë ¥ ë°›ê¸° ---
if question := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    pdf_mode, docs, retriever = False, None, None

    if uploaded_files:
        raws = tuple(f.getvalue() for f in uploaded_files)
        # ìºì‹œ í‚¤ë¡œ ì“¸ ê²°í•© í•´ì‹œ
        combined_hash = hashlib.md5(b"".join(raws)).hexdigest()

        try:
            vectorstore, docs = build_vs_multi(raws)
        except Exception as e:
            st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            pdf_mode = False
        else:
            base_retriever = vectorstore.as_retriever(search_kwargs={"k": 3, "fetch_k": 12})
            base_retriever.search_type = "mmr"

            compressor = LLMChainExtractor.from_llm(llm)
            retriever = ContextualCompressionRetriever(
                base_compressor=compressor, base_retriever=base_retriever
            )
            pdf_mode = True
            st.session_state["last_file_hash"] = combined_hash  # ìš”ì•½/ì›Œí´ ìºì‹œìš©
            st.markdown(
                """
                <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                    <strong>PDFë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</strong>
                </div>
                """,
                unsafe_allow_html=True,
            )
    else:
        st.markdown(
            """
            <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                <strong>PDF ì—†ì´ ê¸°ë³¸ì ì¸ ê¸ˆìœµ ì •ë³´ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.</strong>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.session_state.pop("file_meta", None)
        

    # --- ë‹µë³€ ìƒì„± ---
    if pdf_mode:
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": stuff_prompt},
            return_source_documents=False,
        )

        try:
            result = qa_chain.invoke({"query": question})
            response = (result["result"] or "").strip()
        except Exception:
            response = ""

        
        if not response:
            try:
                rephrased = llm.predict(
                    f"ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ìœ ë¦¬í•˜ê²Œ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬í‘œí˜„í•´ì¤˜: {question}"
                ).strip()
            except Exception:
                response = question

        qa_chain_loose = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=base_retriever,
            chain_type_kwargs={"prompt": stuff_prompt},
            return_source_documents=False,
        )
        try:
            result2 = qa_chain_loose.invoke({"query": rephrased})
            response = (result2["result"] or "").strip()
        except Exception:
            response = ""

        if not response:
            try:
                summary = summarize_once(docs, st.session_state.get("last_file_hash", "nohash"))
                response = "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ë§¤ì¹­ì„ ì°¾ê¸° ì–´ë ¤ì›Œ ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤:\n\n" + summary
            except Exception:
                response = "ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ PDFë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    else:
        prompt = PromptTemplate.from_template("""
            ë„ˆëŠ” ê¸ˆìœµíˆ¬ì ë¶„ì•¼ì— íŠ¹í™”ëœ AIì•¼.
            ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ ê°„ê²°í•˜ê²Œ ë‹µí•´:
            1) ì‹ ë¢° ê°€ëŠ¥í•œ ì¶œì²˜ ê¸°ë°˜  2) ì´ˆë³´ì ìš©ì–´ í’€ì–´ì“°ê¸°  3) ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†Œ í¬í•¨
            ì§ˆë¬¸: {question}
        """)
        response = llm.predict(prompt.format(question=question))

    # ì„¸ì…˜ì— ì €ì¥ + ì´ì „ PDF ì´ˆê¸°í™”
    st.session_state["last_response"] = response
    st.session_state["last_docs"] = docs
    st.session_state["last_file_hash"] = file_hash if pdf_mode else None
    st.session_state.pop("pdf_download", None) # ìƒˆë¡œìš´ ì§ˆë¬¸ë§ˆë‹¤ ì´ì „ pdf ì œê±°



    # --- í•œ ë²ˆë§Œ ì¶œë ¥ ---
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})

    # --- ë¹„ë™ê¸° ë¡œê·¸ ---
    def log_async(q, a):
        def _w():
            conn = sqlite3.connect("chat_logs.db", check_same_thread=False)
            cur = conn.cursor()
            cur.execute("""CREATE TABLE IF NOT EXISTS chat_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT, answer TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )""")
            cur.execute("INSERT INTO chat_logs (question, answer) VALUES (?,?)", (q, a))
            conn.commit(); conn.close()
        threading.Thread(target=_w, daemon=True).start()
    log_async(question, response)


# ì¢Œì¸¡ fAq 
with st.sidebar:
    st.markdown("## ğŸ“œ íˆ¬ì FAQ & ê°€ì´ë“œ")

    with st.expander("ğŸ“Œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸"):
        st.markdown("""
        **1. ETFë€?**  
        ìƒì¥ì§€ìˆ˜í€ë“œë¡œ, ì§€ìˆ˜ë¥¼ ì¶”ì¢…í•˜ëŠ” í€ë“œì…ë‹ˆë‹¤. ì£¼ì‹ì²˜ëŸ¼ ê±°ë˜ë©ë‹ˆë‹¤.

        **2. ETFì™€ í€ë“œ ì°¨ì´?**  
        í€ë“œëŠ” í•˜ë£¨ 1ë²ˆ ê±°ë˜, ETFëŠ” ì‹¤ì‹œê°„ ê±°ë˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

        **3. ì†Œì•¡ìœ¼ë¡œë„ ê°€ëŠ¥í•œê°€ìš”?**  
        ë„¤. ETFëŠ” 1ì£¼ ë‹¨ìœ„ë¡œ ë§¤ìˆ˜í•  ìˆ˜ ìˆì–´, ìˆ˜ì²œ ì›ìœ¼ë¡œë„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        **4. ìˆ˜ìµë¥ ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?**  
        (í˜„ì¬ê°€ - ë§¤ìˆ˜ê°€) / ë§¤ìˆ˜ê°€ Ã— 100
        """)

    with st.expander("ğŸ’¡ ì´ˆë³´ìë¥¼ ìœ„í•œ íˆ¬ì ê°œë…"):
        st.markdown("""
        **ğŸ“ ë¶„ì‚° íˆ¬ì**  
        ì—¬ëŸ¬ ìì‚°ì— ë‚˜ëˆ„ì–´ íˆ¬ìí•¨ìœ¼ë¡œì¨ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì´ëŠ” ì „ëµì…ë‹ˆë‹¤.

        **ğŸ“ ë¦¬ìŠ¤í¬ë€?**  
        ìˆ˜ìµì´ ë“¤ì­‰ë‚ ì­‰í•˜ê±°ë‚˜ ì†ì‹¤ì´ ë‚  ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

        **ğŸ“ ETF ì¥ì **  
        - ë¶„ì‚° íˆ¬ì íš¨ê³¼
        - ë‚®ì€ ìˆ˜ìˆ˜ë£Œ
        - ë‹¤ì–‘í•œ ìì‚°êµ°(ì£¼ì‹, ì±„ê¶Œ, ê¸ˆ ë“±)

        **ğŸ“ ì¥ê¸° íˆ¬ìë€?**  
        ë‹¨ê¸° ì‹œì„¸ ë³€ë™ì— í”ë“¤ë¦¬ì§€ ì•Šê³  ì¼ì • ê¸°ê°„ ì´ìƒ ë³´ìœ í•˜ì—¬ ìˆ˜ìµì„ ê¸°ëŒ€í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.
        """)


# --- í•­ìƒ ë Œë” (PDF ìƒì„±/ë‹¤ìš´ë¡œë“œ)---
can_export = bool(st.session_state.get("last_response"))

if st.session_state.get("do_postprocess") and st.session_state.get("last_docs"):
    @st.cache_data(show_spinner=False)
    def summarize_once(_docs, _hash):
        _ = str(_hash)
        map_prompt = PromptTemplate(
            input_variables=["text"],
            template=(
                "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ì„ í•œêµ­ì–´ë¡œ 3~5ê°œ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½í•˜ë¼.\n"
                "- ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ\n"
                "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n{text}\n\nìš”ì•½:"
            ),
        )
        combine_prompt = PromptTemplate(
            input_variables=["text"],
            template=(
                "ì•„ë˜ ìš”ì•½ë“¤ì„ í•œêµ­ì–´ ë‹¨ë½ 3~5ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ë¼. "
                "ì¤‘ë³µ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¨ë¼.\n\n{text}\n\nìµœì¢… ìš”ì•½:"
            ),
        )
        chain = load_summarize_chain(
            llm, chain_type="map_reduce",
            map_prompt=map_prompt, combine_prompt=combine_prompt
        )
        return chain.run(_docs)

    summary = summarize_once(st.session_state["last_docs"], st.session_state["last_file_hash"])
    st.success(summary)

    @st.cache_data(show_spinner=False)
    def generate_wordcloud_image_cached(text:str, _hash:str):
        _ = str(_hash)
        stop = set(STOPWORDS) | {"https","http","www","com"}
        wc = WordCloud(font_path=font_path, width=800, height=400, background_color="white", collocations=False, stopwords=stop).generate(text)
        buf = BytesIO(); wc.to_image().save(buf, format='PNG'); buf.seek(0); return buf

    hash_key = st.session_state.get("last_file_hash", "nohash")
    st.image(generate_wordcloud_image_cached(summary, hash_key), use_container_width=True)



if st.button("ğŸ¤– ë‹µë³€ì„ PDFë¡œ ì €ì¥", disabled=not can_export, use_container_width=True):
    try:
        from reportlab.platypus import SimpleDocTemplate, Paragraph
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.lib.pagesizes import A4

        text = st.session_state.get("last_response", "")
        buf = BytesIO()

        # A4 í˜ì´ì§€ ì„¤ì •
        doc = SimpleDocTemplate(
            buf,
            pagesize=A4,
            leftMargin=36,
            rightMargin=36,
            topMargin=36,
            bottomMargin=36
        )

        # ìŠ¤íƒ€ì¼ ì„¤ì •
        styles = getSampleStyleSheet()
        kstyle = ParagraphStyle(
            'Korean',
            parent=styles['Normal'],
            fontName='NanumGothic',
            fontSize=12,
            leading=16,
            wordWrap='CJK'
        )

        # ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        text = text.replace("\n", "<br/>")
        story = [Paragraph(text, kstyle)]
        doc.build(story)

        st.session_state["pdf_download"] = buf.getvalue()
        st.toast("PDF ì¤€ë¹„ ì™„ë£Œ âœ…")
    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

pdf_bytes = st.session_state.get("pdf_download")
if pdf_bytes:
    st.download_button(
        "ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ",
        data=pdf_bytes,
        file_name="etf_response.pdf",
        mime="application/pdf",
        use_container_width=True,
        type="primary",
    )