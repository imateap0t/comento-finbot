import hashlib
import os
import sqlite3
import time
from io import BytesIO

import streamlit as st
from dotenv import load_dotenv

# ====== LLM & LangChain (LangChain 0.3.27) ======
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.retrievers import BM25Retriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever
from langchain_core.callbacks import BaseCallbackHandler
from langchain.retrievers import EnsembleRetriever

# ====== PDF/ì‹œê°í™” ======
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.pagesizes import A4
from wordcloud import WordCloud, STOPWORDS

# ====== ê¸°ë³¸ ì„¤ì • ======
load_dotenv()

st.set_page_config(page_title="ETF ì±—ë´‡", page_icon="ğŸ’¹")

api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", None)
if not api_key:
    st.error("""
    ğŸš¨ **OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤**
    
    `.env` íŒŒì¼ì— ë‹¤ìŒê³¼ ê°™ì´ ì¶”ê°€í•´ì£¼ì„¸ìš”:
    ```
    OPENAI_API_KEY=your_api_key_here
    ```
    
    ë˜ëŠ” Streamlit Cloudì—ì„œ Secretsë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
    """)
    st.stop()

# LLM ì„¤ì •
try:
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key=api_key)
    # API í‚¤ ìœ íš¨ì„±
    test_response = llm.invoke("test")
except Exception as e:
    st.error(f"ğŸš¨ OpenAI API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    if "api_key" in str(e).lower():
        st.error("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# UI í—¤ë”
st.title("ğŸ’¹ ê¸ˆìœµ ìƒë‹´ ì±—ë´‡")
st.markdown("ê¸ˆìœµì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# í•œê¸€ í°íŠ¸ ë“±ë¡
font_path = os.path.join(os.path.dirname(__file__), "NanumGothic-Regular.ttf")
pdfmetrics.registerFont(TTFont("NanumGothic", font_path))

# CSS
st.markdown(
    """
    <style>
    .chat-input textarea { height: 3em !important; border-radius: 8px; padding: 10px; font-size: 1rem; }
    .send-button { background-color: #1E90FF; color: white; border-radius: 6px; padding: 0.5em 1.2em; border: none; cursor: pointer; margin-left: 10px; }
    .chat-row { display: flex; align-items: center; gap: 10px; }
    </style>
    """,
    unsafe_allow_html=True,
)

# ====== ì„¸ì…˜ ======
if "messages" not in st.session_state:
    st.session_state.messages = []

# ====== ì—…ë¡œë“œ ìœ„ì ¯ ======
uploaded_files = st.file_uploader("PDF ì—¬ëŸ¬ ê°œ ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True)
st.markdown("##### ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

# ====== ë²¡í„°ìŠ¤í† ì–´ ë¹Œë” (ë©€í‹° PDF, ê²¬ê³ í™”) ======
@st.cache_resource(show_spinner=False)
def build_vs_multi(
    files: tuple[bytes, ...],
    file_names: tuple[str, ...],
    chunk_size: int = 1000,
    chunk_overlap: int = 150,
    embed_model: str = "text-embedding-3-small",
):
    # ìºì‹œ í‚¤ ì•ˆì •í™”: íŒŒì¼ í•´ì‹œ + íŒŒë¼ë¯¸í„°
    _ = (chunk_size, chunk_overlap, embed_model, tuple(hashlib.md5(b).hexdigest() for b in files))

    all_docs = []
    tmp_paths = []
    try:
        for raw, fname in zip(files, file_names):
            if not raw:
                continue
            h = hashlib.md5(raw).hexdigest()
            path = f"/tmp/{h}.pdf"
            with open(path, "wb") as f:
                f.write(raw)
            tmp_paths.append(path)

            # PDF ë¡œë“œ (ì•”í˜¸í™”/ê¹¨ì§ ì˜ˆì™¸ ì²˜ë¦¬)
            try:
                pages = PyPDFLoader(path).load()
                if not pages:
                    st.warning(f"âš ï¸ '{fname}': ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue
                total_text = "".join(page.page_content for page in pages)
                if len(total_text.strip()) < 50:
                    st.warning(f"âš ï¸ '{fname}': í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë µìŠµë‹ˆë‹¤.")
                    continue

            except Exception as e:
                error_msg = str(e).lower()
                if "encrypted" in error_msg or "password" in error_msg:
                    st.warning(f"ğŸ”’ '{fname}': ì•”í˜¸í™”ëœ PDFì…ë‹ˆë‹¤. ì•”í˜¸ë¥¼ í•´ì œí•˜ê³  ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                elif "damaged" in error_msg or "corrupted" in error_msg:
                    st.warning(f"ğŸ’¥ '{fname}': ì†ìƒëœ íŒŒì¼ì…ë‹ˆë‹¤.")
                elif "permission" in error_msg:
                    st.warning(f"ğŸš« '{fname}': íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning(f"âŒ '{fname}' ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue

            splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            docs = splitter.split_documents(pages)
            for d in docs:
                d.metadata = {**d.metadata, "source_file": fname}
            all_docs.extend(docs)

        if not all_docs:
            raise ValueError("ìœ íš¨í•œ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

        try:
            if not all_docs:
                raise ValueError("ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            vs = FAISS.from_documents(all_docs, OpenAIEmbeddings(model=embed_model))
            st.success(f"âœ… {len(all_docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ğŸš¨ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            if "api" in str(e).lower():
                st.error("OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. API í‚¤ì™€ ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise e
        
        return vs, all_docs

    finally:
        # ì„ì‹œíŒŒì¼ ì •ë¦¬
        for p in tmp_paths:
            try:
                os.remove(p)
            except Exception:
                pass

# ====== í”„ë¡¬í”„íŠ¸ ======
stuff_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "ë„ˆëŠ” í•œêµ­ì–´ ê¸ˆìœµ ì „ë¬¸ê°€ì•¼. ì•„ë˜ 'ì»¨í…ìŠ¤íŠ¸'ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì°¾ì•„ ë‹µí•´.\n"
        "- í•œêµ­ì–´ë§Œ ì‚¬ìš©í•  ê²ƒ\n"
        "- ì´ˆë³´ìë„ ì´í•´ ê°€ëŠ¥í•˜ê²Œ ë‹¨ê³„ì ìœ¼ë¡œ ì„¤ëª…\n"
        "- ìˆ˜ì¹˜/ì „ëµ/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ(ê¸°ê°„, ì¡°ê±´ í¬í•¨)\n"
        "- ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ 'ëª¨ë¥¸ë‹¤'ê³  ë‹µí•˜ê³  ì¶”ì¸¡í•˜ì§€ ë§ ê²ƒ\n"
        "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n"
        "ì»¨í…ìŠ¤íŠ¸:\n{context}\n\n"
        "ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    ),
)

# ====== ìš”ì•½ í•¨ìˆ˜ ======
@st.cache_data(show_spinner=False)
def summarize_once(_docs, _hash):
    _ = str(_hash)
    map_prompt = PromptTemplate(
        input_variables=["text"],
        template=(
            "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ì„ í•œêµ­ì–´ë¡œ 3~5ê°œ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½í•´.\n"
            "- ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†ŒëŠ” êµ¬ì²´ì ìœ¼ë¡œ\n"
            "- ê³¼ë„í•œ í™•ì • í‘œí˜„ ê¸ˆì§€\n\n{text}\n\nìš”ì•½:"
        ),
    )
    combine_prompt = PromptTemplate(
        input_variables=["text"],
        template=(
            "ì•„ë˜ ìš”ì•½ë“¤ì„ í•œêµ­ì–´ ë‹¨ë½ 3~5ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•´. ì¤‘ë³µì€ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¨.\n\n{text}\n\nìµœì¢… ìš”ì•½:"
        ),
    )
    chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=map_prompt, combine_prompt=combine_prompt)
    return chain.run(_docs)

# ====== ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ =======
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""
    def on_llm_start(self, *args, **kwargs):
        self.text = ""
        if self.container:
            self.container.markdown("")
    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        if self.container:
            self.container.markdown(self.text)
    def on_llm_end(self, *args, **kwargs):
        pass


# ====== ê³¼ê±° ë©”ì‹œì§€ ë Œë” ======
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# í† ê¸€
with st.sidebar:
    st.markdown("---")
    st.subheader("âš™ï¸ ì¶”ê°€ ê¸°ëŠ¥")
    
    # ìš”ì•½/ì´ë¯¸ì§€ ìƒì„± í† ê¸€
    do_post = st.toggle(
        "ğŸ“Š ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œ ìë™ ìƒì„±", 
        key="do_postprocess", 
        value=False,
        help="PDF ë¬¸ì„œê°€ ì—…ë¡œë“œëœ ìƒíƒœì—ì„œ ë‹µë³€ ì‹œ ìë™ìœ¼ë¡œ ë¬¸ì„œ ìš”ì•½ê³¼ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í† ê¸€
    stream_on = st.toggle(
        "âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°", 
        key="do_stream", 
        value=True,
        help="ë‹µë³€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ë„ë©´ ì™„ì„±ëœ ë‹µë³€ì„ í•œ ë²ˆì— ë³´ì—¬ì¤ë‹ˆë‹¤."
    )
    
    # í† ê¸€ ìƒíƒœ í‘œì‹œ
    if do_post:
        if st.session_state.get("last_docs"):
            st.success("âœ… ë‹¤ìŒ ë‹µë³€ë¶€í„° ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤")
        else:
            st.info("â„¹ï¸ PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ë©´ ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤")

# ====== ì›Œë“œí´ë¼ìš°ë“œ ======
@st.cache_data(show_spinner=False)
def generate_wordcloud_image_cached(text: str, _hash: str):
    """ì›Œë“œí´ë¼ìš°ë“œ ì´ë¯¸ì§€ ìƒì„± (ìºì‹œë¨)"""
    _ = str(_hash)
    
    # í•œê¸€ ë¶ˆìš©ì–´ í™•ì¥
    korean_stopwords = {
        "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë˜í•œ", "ë”°ë¼ì„œ", "ê·¸ë˜ì„œ", "ì¦‰", "ì˜ˆë¥¼ ë“¤ì–´",
        "ìˆìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ê²ƒì…ë‹ˆë‹¤", "ìˆ˜", "ë•Œ", "ë“±", "í†µí•´",
        "ëŒ€í•œ", "ìœ„í•œ", "ê´€ë ¨", "ê²½ìš°", "ë°©ë²•", "ì´ëŸ°", "ê·¸ëŸ°", "ì´ì™€", "ê°™ì€"
    }
    
    stop = set(STOPWORDS) | {"https", "http", "www", "com"} | korean_stopwords

    try:
        wc = WordCloud(
            font_path=font_path, 
            width=800, 
            height=400, 
            background_color="white", 
            collocations=False, 
            stopwords=stop,
            max_words=100,
            colormap='viridis'
        ).generate(text)
        
        buf = BytesIO()
        wc.to_image().save(buf, format="PNG")
        buf.seek(0)
        return buf
        
    except Exception as e:
        st.error(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# ====== ë©”ì¸ ì…ë ¥ ======
if question := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state.messages.append({"role": "user", "content": question})

    # í™”ë©´ ìë¦¬ + ì½œë°±
    live_area = st.empty()
    handler = StreamHandler(live_area) if st.session_state.get("do_stream", False) else None
    cfg = {"callbacks": [handler]} if handler else {}

    st.session_state["last_sources"] = []

    pdf_mode, docs, retriever, base_retriever = False, None, None, None

    if uploaded_files:
        raws = tuple(f.getvalue() for f in uploaded_files if f is not None)
        names = tuple(f.name for f in uploaded_files if f is not None)
        combined_hash = hashlib.md5(b"".join(raws)).hexdigest() if raws else "nohash"

        pdf_mode, docs, retriever, base_retriever = False, None, None, None

        try:
            with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                vectorstore, docs = build_vs_multi(
                    raws, names, 
                    chunk_size=1000, 
                    chunk_overlap=150, 
                    embed_model="text-embedding-3-small"
                )
        except ValueError as e:
            st.error(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            st.info("ğŸ’¡ ë‹¤ë¥¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDFì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            pdf_mode = False
        except Exception as e:
            st.error(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.info("ğŸ’¡ íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            pdf_mode = False
        else:
            # Dense(ì„ë² ë”©) + Keyword(BM25) í•˜ì´ë¸Œë¦¬ë“œ
            dense = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 3, "fetch_k": 12})
            bm25 = BM25Retriever.from_documents(docs)
            ensemble = EnsembleRetriever(retrievers=[bm25, dense], weights=[0.35, 0.65])

            compressor = LLMChainExtractor.from_llm(llm)
            retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble)

            base_retriever = dense
            pdf_mode = True
            st.session_state["last_file_hash"] = combined_hash
            st.markdown(
                """
                <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                    <strong>PDFë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.</strong>
                </div>
                """,
                unsafe_allow_html=True,
            )
    else:
        st.markdown(
            """
            <div style="background-color:#3E3B16;padding:10px;border-radius:5px;border-left:5px solid #FFD700;">
                <strong>PDF ì—†ì´ ê¸°ë³¸ì ì¸ ê¸ˆìœµ ì •ë³´ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.</strong>
            </div>
            """,
            unsafe_allow_html=True,
        )
        st.session_state.pop("file_meta", None)


    # ====== ë‹µë³€ ìƒì„± ======
    if pdf_mode and retriever:
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": stuff_prompt},
            return_source_documents=True,
        )

        response = ""
        sources = []
        try:
            with st.spinner("ğŸ” ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
                res = qa_chain.invoke({"query": question}, config=cfg)
                response = (res.get("result") or "").strip()
                sources = res.get("source_documents", []) or []
                
        except Exception as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg:
                st.error("â³ API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                response = "ì ì‹œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            elif "timeout" in error_msg:
                st.error("â±ï¸ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ í•´ë³´ì„¸ìš”.")
                response = "ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ í•´ì£¼ì‹œê² ì–´ìš”?"
            elif "token" in error_msg:
                st.error("ğŸ“ ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ì–´ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì‘ì€ PDFë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
                response = "ë¬¸ì„œê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ë” ì‘ì€ ë¬¸ì„œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                st.error(f"ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            sources = []

        # ì†ŒìŠ¤ ì „ì²˜ë¦¬ í›„ ì„¸ì…˜ ì €ì¥
        processed_sources = []
        _seen = set()
        for d in sources:
            src_name = d.metadata.get("source_file") or os.path.basename(d.metadata.get("source", "?"))
            pg = d.metadata.get("page")
            pg_num = (pg + 1) if isinstance(pg, int) else "?"  # PyPDFLoaderëŠ” 0-index ê°€ëŠ¥ì„±
            key = (src_name, pg_num)
            if key in _seen:
                continue
            _seen.add(key)
            snippet = (d.page_content or "").strip()
            if len(snippet) > 400:
                snippet = snippet[:400] + "â€¦"
            processed_sources.append({"file": src_name, "page": pg_num, "snippet": snippet})

        st.session_state["last_sources"] = processed_sources


        if not response:
            try:
                r = llm.invoke(f"ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ìœ ë¦¬í•˜ê²Œ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬í‘œí˜„í•´ì¤˜: {question}")
                rephrased = getattr(r, "content", str(r)).strip()
            except Exception:
                rephrased = question

            qa_chain_loose = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=base_retriever if base_retriever else retriever,
                chain_type_kwargs={"prompt": stuff_prompt},
                return_source_documents=True,
            )
            try:
                result2 = qa_chain_loose.invoke({"query": rephrased or question}, config=cfg)
                response = (result2.get("result") or "").strip()
            except Exception:
                response = ""

        if not response:
            try:
                summary = summarize_once(docs, st.session_state.get("last_file_hash", "nohash"))
                response = "ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ë§¤ì¹­ì„ ì°¾ê¸° ì–´ë ¤ì›Œ ìš”ì•½ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤:\n\n" + summary
            except Exception:
                response = "ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ PDFë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    else:
        prompt = PromptTemplate.from_template(
            """
            ë„ˆëŠ” ê¸ˆìœµíˆ¬ì ë¶„ì•¼ì— íŠ¹í™”ëœ AIì•¼.
            ì•„ë˜ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ì–´ë¡œë§Œ ê°„ê²°í•˜ê²Œ ë‹µí•´:
            1) ì‹ ë¢° ê°€ëŠ¥í•œ ì¶œì²˜ ê¸°ë°˜  2) ì´ˆë³´ì ìš©ì–´ í’€ì–´ì“°ê¸°  3) ìˆ˜ì¹˜/ìœ„í—˜ìš”ì†Œ í¬í•¨
            ì§ˆë¬¸: {question}
            """
        )
        try:
            with st.spinner("ğŸ’­ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                r = llm.invoke(prompt.format(question=question), config=cfg)
                response = getattr(r, "content", str(r))
        except Exception as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg:
                response = "â³ ìš”ì²­ì´ ë§ì•„ ì ì‹œ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤. 1ë¶„ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                st.warning("API ìš”ì²­ í•œë„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            elif "api_key" in error_msg:
                response = "ğŸ”‘ API í‚¤ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                st.error("API í‚¤ ì˜¤ë¥˜ ë°œìƒ")
            else:
                response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                st.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")

with st.chat_message("assistant"):
    st.markdown(response)
    
    # ë¬¸ì„œ í‘œì‹œ (ìˆì„ ë•Œë§Œ)
    if st.session_state.get("last_sources"):
        with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
            for i, s in enumerate(st.session_state["last_sources"], 1):
                st.markdown(f"**{i}. {s['file']} â€” p.{s['page']}**")
                st.code(s["snippet"])

    # ====== ì‹¤ì‹œê°„ ìš”ì•½/ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ======
    if st.session_state.get("do_postprocess") and st.session_state.get("last_docs"):
        st.markdown("---")
        st.subheader("ğŸ“Š ë¬¸ì„œ ìš”ì•½ ë° ì‹œê°í™”")
        
        try:
            with st.spinner("ğŸ“ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                summary = summarize_once(
                    st.session_state["last_docs"], 
                    st.session_state.get("last_file_hash", "nohash")
                )
            
            # ìš”ì•½ í‘œì‹œ
            st.success("**ğŸ“‹ ë¬¸ì„œ ìš”ì•½:**")
            st.info(summary)

            # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
            try:
                with st.spinner("ğŸ¨ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    hash_key = st.session_state.get("last_file_hash", "nohash")
                    wordcloud_img = generate_wordcloud_image_cached(summary, hash_key)
                
                st.success("**â˜ï¸ í‚¤ì›Œë“œ í´ë¼ìš°ë“œ:**")
                st.image(wordcloud_img, use_container_width=True, caption="ë¬¸ì„œì˜ ì£¼ìš” í‚¤ì›Œë“œ")
                
            except Exception as e:
                st.warning(f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            st.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
    
    st.session_state.messages.append({"role": "assistant", "content": response})


    # ====== ë Œë” & ì„¸ì…˜ ì €ì¥ ======
    st.session_state["last_response"] = response
    if pdf_mode and docs:
        st.session_state["last_docs"] = docs
        st.session_state["last_file_hash"] = combined_hash
    else:
        st.session_state.pop("last_docs", None)

    try:
        if handler:
            live_area.empty()
    except Exception:
        pass

    with st.chat_message("assistant"):
        st.markdown(response)
        # ë¬¸ì„œ í‘œì‹œ (ìˆì„ ë•Œë§Œ)
        if st.session_state.get("last_sources"):
            with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
                for i, s in enumerate(st.session_state["last_sources"], 1):
                    st.markdown(f"**{i}. {s['file']} â€” p.{s['page']}**")
                    st.code(s["snippet"])
        st.session_state.messages.append({"role": "assistant", "content": response})

    
        # ë¹„ë™ê¸° ë¡œê·¸ ì €ì¥
        import threading

        def log_async(q, a):
            def _w():
                conn = sqlite3.connect("chat_logs.db", check_same_thread=False)
                cur = conn.cursor()
                cur.execute(
                    """
                    CREATE TABLE IF NOT EXISTS chat_logs (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        question TEXT, answer TEXT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                    )
                    """
                )
                cur.execute("INSERT INTO chat_logs (question, answer) VALUES (?,?)", (q, a))
                conn.commit()
                conn.close()
            threading.Thread(target=_w, daemon=True).start()

        log_async(question, response)

# ====== ì‚¬ì´ë“œë°” ======
with st.sidebar:
    st.markdown("## ğŸ“œ íˆ¬ì FAQ & ê°€ì´ë“œ")
    with st.expander("ğŸ“Œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸"):
        st.markdown(
            """
            **1. ETFë€?**  
            ìƒì¥ì§€ìˆ˜í€ë“œë¡œ, ì§€ìˆ˜ë¥¼ ì¶”ì¢…í•˜ëŠ” í€ë“œì…ë‹ˆë‹¤. ì£¼ì‹ì²˜ëŸ¼ ê±°ë˜ë©ë‹ˆë‹¤.

            **2. ETFì™€ í€ë“œ ì°¨ì´?**  
            í€ë“œëŠ” í•˜ë£¨ 1ë²ˆ ê±°ë˜, ETFëŠ” ì‹¤ì‹œê°„ ê±°ë˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

            **3. ì†Œì•¡ìœ¼ë¡œë„ ê°€ëŠ¥í•œê°€ìš”?**  
            ë„¤. ETFëŠ” 1ì£¼ ë‹¨ìœ„ë¡œ ë§¤ìˆ˜í•  ìˆ˜ ìˆì–´, ìˆ˜ì²œ ì›ìœ¼ë¡œë„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

            **4. ìˆ˜ìµë¥ ì€ ì–´ë–»ê²Œ ê³„ì‚°í•˜ë‚˜ìš”?**  
            (í˜„ì¬ê°€ - ë§¤ìˆ˜ê°€) / ë§¤ìˆ˜ê°€ Ã— 100
            """
        )
    with st.expander("ğŸ’¡ ì´ˆë³´ìë¥¼ ìœ„í•œ íˆ¬ì ê°œë…"):
        st.markdown(
            """
            **ğŸ“ ë¶„ì‚° íˆ¬ì**  
            ì—¬ëŸ¬ ìì‚°ì— ë‚˜ëˆ„ì–´ íˆ¬ìí•¨ìœ¼ë¡œì¨ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì´ëŠ” ì „ëµì…ë‹ˆë‹¤.

            **ğŸ“ ë¦¬ìŠ¤í¬ë€?**  
            ìˆ˜ìµì´ ë“¤ì­‰ë‚ ì­‰í•˜ê±°ë‚˜ ì†ì‹¤ì´ ë‚  ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

            **ğŸ“ ETF ì¥ì **  
            - ë¶„ì‚° íˆ¬ì íš¨ê³¼  
            - ë‚®ì€ ìˆ˜ìˆ˜ë£Œ  
            - ë‹¤ì–‘í•œ ìì‚°êµ°(ì£¼ì‹, ì±„ê¶Œ, ê¸ˆ ë“±)

            **ğŸ“ ì¥ê¸° íˆ¬ìë€?**  
            ë‹¨ê¸° ì‹œì„¸ ë³€ë™ì— í”ë“¤ë¦¬ì§€ ì•Šê³  ì¼ì • ê¸°ê°„ ì´ìƒ ë³´ìœ í•˜ì—¬ ìˆ˜ìµì„ ê¸°ëŒ€í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.
            """
        )

with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ”„ ëŒ€í™” ê´€ë¦¬")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì‚­ì œ", use_container_width=True):
            st.session_state.messages = []
            st.session_state.pop("last_response", None)
            st.session_state.pop("last_sources", None)
            st.rerun()
    
    with col2:
        if st.button("ğŸ’¾ ëŒ€í™” ì €ì¥", use_container_width=True):
            if st.session_state.messages:
                # ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                conversation = ""
                for msg in st.session_state.messages:
                    role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì±—ë´‡"
                    conversation += f"{role}: {msg['content']}\n\n"
                
                st.download_button(
                    "ğŸ“„ ëŒ€í™” ë‚´ì—­ ë‹¤ìš´ë¡œë“œ",
                    data=conversation.encode('utf-8'),
                    file_name=f"ëŒ€í™”ê¸°ë¡_{time.strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    key="download_chat"
                )

# ====== í¬ìŠ¤íŠ¸í”„ë¡œì„¸ì‹± ìœ„ì ¯ ======

# ====== PDF ì €ì¥ ======
if st.button("ğŸ¤– ë‹µë³€ì„ PDFë¡œ ì €ì¥", disabled=not can_export, use_container_width=True):
    try:
        text = st.session_state.get("last_response", "")
        buf = BytesIO()
        doc = SimpleDocTemplate(
            buf,
            pagesize=A4,
            leftMargin=36,
            rightMargin=36,
            topMargin=36,
            bottomMargin=36,
        )
        styles = getSampleStyleSheet()
        kstyle = ParagraphStyle(
            "Korean",
            parent=styles["Normal"],
            fontName="NanumGothic",
            fontSize=12,
            leading=16,
            wordWrap="CJK",
        )
        text = text.replace("\n", "<br/>")
        story = [Paragraph(text, kstyle)]
        doc.build(story)
        st.session_state["pdf_download"] = buf.getvalue()
        st.toast("PDF ì¤€ë¹„ ì™„ë£Œ âœ…")
    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

pdf_bytes = st.session_state.get("pdf_download")
if pdf_bytes:
    st.download_button(
        "ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ",
        data=pdf_bytes,
        file_name="etf_response.pdf",
        mime="application/pdf",
        use_container_width=True,
        type="primary",
    )
